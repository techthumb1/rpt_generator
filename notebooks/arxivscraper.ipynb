{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper as ax\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# scraper for arxiv stat.ml\n",
    "scraper = ax.Scraper(category='stat', date_from='2017-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['stat.ml'],'abstract':['learning']})\n",
    "\n",
    "# scraper for arxiv q-bio\n",
    "scraper = ax.Scraper(category='q-bio', date_from='2016-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['q-bio.GN', 'q-bio.NC']})\n",
    "'''\n",
    "\n",
    "# scraper for arxiv physics\n",
    "scraper = ax.Scraper(category='physics', date_from='2019-05-01',\n",
    "                     date_until='2019-07-03', t=10,\n",
    "                     filters={'categories':['quant-ph']})\n",
    "\n",
    "output = scraper.scrape()\n",
    "\n",
    "\n",
    "\n",
    "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "titles = [' '.join(o['title'].split()) for o in output]\n",
    "np.savetxt('titles_ref.csv', np.array(titles), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also choose another object to scrape, such as Twitter\n",
    "import urllib\n",
    "import numpy as np\n",
    "\n",
    "#Scrape some interesting quotes\n",
    "url = 'https://raw.githubusercontent.com/akhiltak/inspirational-quotes/master/Quotes.csv'\n",
    "response = urllib.request.urlopen(url).read().decode()\n",
    "quotes = []\n",
    "lines = response.split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    quotes.append(line.split(';')[0].replace(\"\\'\", '').replace('*', '').replace('#', '').replace('%', '').replace('&', ''))\n",
    "    \n",
    "np.savetxt('titles.csv', np.array(quotes[1:]), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Twitter credentials to scrape tweets\n",
    "import tweetscraper\n",
    "# Tweetscraper.get_all_tweets(\"SICKOFWOLVES\") # name of account to scrape\n",
    "tweetscraper.clean_csv(fname='data/wolves_tweets.csv') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune gp2\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"117M\" # \"355M\" for larger model\n",
    "gpt2.download_gpt2(model_name=model_name) # Model is saved at ~/.gpt-2/models/117M/\n",
    "\n",
    "session = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess=session,\n",
    "                dataset=quotes,\n",
    "                model_name=model_name,\n",
    "                steps=1000,\n",
    "                restore_from='fresh',\n",
    "                run_name='run1',\n",
    "                print_every=10,\n",
    "                sample_every=100,\n",
    "                save_every=1000)\n",
    "\n",
    "gpt2.generate(sess=session,\n",
    "                length=100,\n",
    "                temperature=0.7,\n",
    "                nsamples=1,\n",
    "                batch_size=1,\n",
    "                prefix=\"\",\n",
    "                truncate='\n",
    "                ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go over some samples of the generated text\n",
    "sample = 'samples/samples-901'\n",
    "t = open(sample, 'r').read()\n",
    "\n",
    "for samp in ['endoftext', 'startoftext', '<|', '|>']:\n",
    "    t = t.replace(samp, '')\n",
    "for title in t.title().split('\\n')[1:]:\n",
    "    if not title == '':\n",
    "        print('- ' + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples from the model\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'neural' # None is default\n",
    "text = gpt2.generate(sess,\n",
    "              length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=neural,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gpt2.generate(sess,\n",
    "            #length=40,\n",
    "            temperature=0.7,\n",
    "            prefix=None,\n",
    "            nsamples=100,\n",
    "            batch_size=1,\n",
    "            return_as_list=True\n",
    "            )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
